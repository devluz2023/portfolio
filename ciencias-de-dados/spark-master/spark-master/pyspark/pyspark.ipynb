{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devluz2023/spark/blob/master/pyspark/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "06e25481",
      "metadata": {
        "id": "06e25481"
      },
      "outputs": [],
      "source": [
        "# instalar as dependÃªncias\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7328b37c",
      "metadata": {
        "id": "7328b37c"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee474d39",
      "metadata": {
        "id": "ee474d39"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff019b84",
      "metadata": {
        "id": "ff019b84"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.master('local[*]').appName(\"Iniciando com Spark\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a1d8387",
      "metadata": {
        "id": "7a1d8387"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a417878",
      "metadata": {
        "id": "4a417878"
      },
      "outputs": [],
      "source": [
        "data = [('zeca', '35'), ('eva', '29')]\n",
        "colNames = ['Nome', 'Idade']\n",
        "#criar um dataframe\n",
        "df = spark.createDataFrame(data, colNames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e091d2a",
      "metadata": {
        "id": "4e091d2a"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea32ea4",
      "metadata": {
        "id": "7ea32ea4"
      },
      "outputs": [],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2c7666",
      "metadata": {
        "id": "9c2c7666"
      },
      "outputs": [],
      "source": [
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de0e29c",
      "metadata": {
        "id": "3de0e29c"
      },
      "outputs": [],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411d5a51",
      "metadata": {
        "id": "411d5a51"
      },
      "outputs": [],
      "source": [
        "workdir = os.getcwd()\n",
        "extract = workdir + '\\dataset'\n",
        "arquivo = extract + '\\empresas.zip'\n",
        "zipfile.ZipFile(arquivo, 'r').extractall(extract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d1a8a9",
      "metadata": {
        "id": "60d1a8a9"
      },
      "outputs": [],
      "source": [
        "empresa_path = extract + '\\empresas'\n",
        "empresas = spark.read.csv(empresa_path, sep=';', inferSchema=True)\n",
        "estabelecimentos_path = extract + '\\estabelecimentos'\n",
        "estabelecimentos = spark.read.csv(estabelecimentos_path, sep=';', inferSchema=True)\n",
        "socios_path = extract + '\\socios'\n",
        "socios = spark.read.csv(socios_path, sep=';', inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a792c4c",
      "metadata": {
        "id": "9a792c4c"
      },
      "outputs": [],
      "source": [
        "print(empresas.count())\n",
        "print(estabelecimentos.count())\n",
        "print(socios.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57978d08",
      "metadata": {
        "id": "57978d08"
      },
      "outputs": [],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf35fbb0",
      "metadata": {
        "id": "cf35fbb0"
      },
      "outputs": [],
      "source": [
        "empresasColNames = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c740056",
      "metadata": {
        "id": "0c740056"
      },
      "outputs": [],
      "source": [
        "estabsColNames = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd82220a",
      "metadata": {
        "id": "dd82220a"
      },
      "outputs": [],
      "source": [
        "sociosColNames = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0631a788",
      "metadata": {
        "id": "0631a788"
      },
      "outputs": [],
      "source": [
        "for index, colName in enumerate(empresasColNames):\n",
        "    empresas = empresas.withColumnRenamed(f\"_c{index}\", colName)\n",
        "empresas.columns\n",
        "\n",
        "for index, colName in enumerate(estabsColNames):\n",
        "    estabelecimentos = estabelecimentos.withColumnRenamed(f\"_c{index}\", colName)\n",
        "estabelecimentos.columns\n",
        "\n",
        "for index, colName in enumerate(sociosColNames):\n",
        "    socios = socios.withColumnRenamed(f\"_c{index}\", colName)\n",
        "socios.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "975f9640",
      "metadata": {
        "id": "975f9640"
      },
      "outputs": [],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534299c9",
      "metadata": {
        "id": "534299c9"
      },
      "outputs": [],
      "source": [
        "socios.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c63e5712",
      "metadata": {
        "id": "c63e5712"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67740f79",
      "metadata": {
        "id": "67740f79"
      },
      "outputs": [],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4350dc98",
      "metadata": {
        "id": "4350dc98"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf1401e",
      "metadata": {
        "id": "4bf1401e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType, StringType\n",
        "from pyspark.sql import functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458b94bc",
      "metadata": {
        "id": "458b94bc"
      },
      "outputs": [],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b135f337",
      "metadata": {
        "id": "b135f337"
      },
      "outputs": [],
      "source": [
        "empresas = empresas.withColumn('capital_social_da_empresa', f.regexp_replace('capital_social_da_empresa', ',', '.'))\n",
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69fda276",
      "metadata": {
        "id": "69fda276"
      },
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad548bfa",
      "metadata": {
        "id": "ad548bfa"
      },
      "outputs": [],
      "source": [
        "empresas = empresas.withColumn('capital_social_da_empresa', empresas['capital_social_da_empresa'].cast(DoubleType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "621cb500",
      "metadata": {
        "id": "621cb500"
      },
      "outputs": [],
      "source": [
        "empresas.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577c023a",
      "metadata": {
        "id": "577c023a"
      },
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54bea202",
      "metadata": {
        "id": "54bea202"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([(20200924,), (20201022,), (20210215,)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb0a44e7",
      "metadata": {
        "id": "cb0a44e7"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82cbf7e0",
      "metadata": {
        "id": "82cbf7e0"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"data\", f.to_date(df.data.cast(StringType()), 'yyyyMMdd'))\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b3c77d2",
      "metadata": {
        "id": "2b3c77d2"
      },
      "outputs": [],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee95a06",
      "metadata": {
        "id": "0ee95a06"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26719723",
      "metadata": {
        "id": "26719723"
      },
      "outputs": [],
      "source": [
        "estabelecimentos = estabelecimentos\\\n",
        "    .withColumn(\n",
        "        \"data_situacao_cadastral\",\n",
        "         f.to_date(estabelecimentos.data_situacao_cadastral.cast(StringType()), 'yyyyMMdd')\n",
        "    )\\\n",
        "    .withColumn(\n",
        "        \"data_de_inicio_atividade\",\n",
        "        f.to_date(estabelecimentos.data_de_inicio_atividade.cast(StringType()), 'yyyyMMdd')\n",
        "    )\\\n",
        "    .withColumn(\n",
        "        \"data_da_situacao_especial\",\n",
        "        f.to_date(estabelecimentos.data_da_situacao_especial.cast(StringType()), 'yyyMMdd')\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5f4512c",
      "metadata": {
        "id": "d5f4512c"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b75394",
      "metadata": {
        "id": "79b75394"
      },
      "outputs": [],
      "source": [
        "estabelecimentos.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b295ef",
      "metadata": {
        "id": "57b295ef"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .select('*')\\\n",
        "    .show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc87d9af",
      "metadata": {
        "id": "dc87d9af"
      },
      "outputs": [],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24992a9e",
      "metadata": {
        "id": "24992a9e"
      },
      "outputs": [],
      "source": [
        "socios.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25cc5405",
      "metadata": {
        "id": "25cc5405"
      },
      "outputs": [],
      "source": [
        "socios = socios\\\n",
        "    .withColumn(\n",
        "        \"data_de_entrada_sociedade\",\n",
        "         f.to_date(socios.data_de_entrada_sociedade.cast(StringType()), 'yyyyMMdd')\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76d843d",
      "metadata": {
        "id": "f76d843d"
      },
      "outputs": [],
      "source": [
        "socios\\\n",
        "    .select('nome_do_socio_ou_razao_social', 'faixa_etaria', f.year('data_de_entrada_sociedade').alias('ano_de_entrada'))\\\n",
        "    .show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3556039",
      "metadata": {
        "id": "e3556039"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([(1,), (2,), (3,), (None,)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c406d65",
      "metadata": {
        "id": "9c406d65"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7bbc45",
      "metadata": {
        "id": "fe7bbc45"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([(1.,), (2.,), (3.,), (float('nan'),)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "883160fb",
      "metadata": {
        "id": "883160fb"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13111179",
      "metadata": {
        "id": "13111179"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([('1',), ('2',), ('3',), (None,)], ['data'])\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76798d69",
      "metadata": {
        "id": "76798d69"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010ee757",
      "metadata": {
        "id": "010ee757"
      },
      "outputs": [],
      "source": [
        "#contagem dos nulos\n",
        "socios.select([f.count(f.when(f.isnull(c), 1)).alias(c) for c in socios.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1183dd7",
      "metadata": {
        "id": "e1183dd7"
      },
      "outputs": [],
      "source": [
        "socios.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5101dd70",
      "metadata": {
        "id": "5101dd70"
      },
      "outputs": [],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2eeec7d",
      "metadata": {
        "id": "a2eeec7d"
      },
      "outputs": [],
      "source": [
        "socios.na.fill(0).limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f48331",
      "metadata": {
        "id": "73f48331"
      },
      "outputs": [],
      "source": [
        "socios.na.fill('-').limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d474722",
      "metadata": {
        "id": "6d474722"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .where(\"capital_social_da_empresa==50\")\\\n",
        "    .show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "681efcbe",
      "metadata": {
        "id": "681efcbe"
      },
      "outputs": [],
      "source": [
        "socios.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6ba7b1",
      "metadata": {
        "id": "9a6ba7b1"
      },
      "outputs": [],
      "source": [
        "socios\\\n",
        "    .select(\"nome_do_socio_ou_razao_social\")\\\n",
        "    .filter(socios.nome_do_socio_ou_razao_social.startswith(\"RODRIGO\"))\\\n",
        "    .filter(socios.nome_do_socio_ou_razao_social.endswith(\"DIAS\"))\\\n",
        "    .limit(50)\\\n",
        "    .toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d9fcb6",
      "metadata": {
        "id": "33d9fcb6"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([('RESTAURANTE DO RUI',),('juca restaurante ltda',), ('Joca restaurante',)],['data'])\n",
        "df\\\n",
        "    .where(f.upper(df.data).like('%RESTAURANTE%'))\\\n",
        "    .show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a93df23b",
      "metadata": {
        "id": "a93df23b"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .select('razao_social_nome_empresarial', 'natureza_juridica', 'porte_da_empresa', 'capital_social_da_empresa')\\\n",
        "    .filter(f.upper(empresas['razao_social_nome_empresarial']).like(\"%RESTAURANTES%\"))\\\n",
        "    .show(15, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6125f358",
      "metadata": {
        "id": "6125f358"
      },
      "source": [
        "# ComeÃ§ando o Trabalho\n",
        "---\n",
        "\n",
        "## Apache Spark - IntroduÃ§Ã£o\n",
        "\n",
        "### [Apache Spark](https://spark.apache.org/)\n",
        "\n",
        "Apache Spark Ã© uma plataforma de computaÃ§Ã£o em *cluster* que fornece uma API para programaÃ§Ã£o distribuÃ­da para processamento de dados em larga escala, semelhante ao modelo *MapReduce*, mas projetada para ser rÃ¡pida para consultas interativas e algoritmos iterativos.\n",
        "\n",
        "O Spark permite que vocÃª distribua dados e tarefas em clusters com vÃ¡rios nÃ³s. Imagine cada nÃ³ como um computador separado. A divisÃ£o dos dados torna mais fÃ¡cil o trabalho com conjuntos de dados muito grandes porque cada nÃ³ funciona processa apenas uma parte parte do volume total de dados.\n",
        "\n",
        "O Spark Ã© amplamente utilizado em projetos analÃ­ticos nas seguintes frentes:\n",
        "\n",
        "- PreparaÃ§Ã£o de dados\n",
        "- Modelos de machine learning\n",
        "- AnÃ¡lise de dados em tempo real\n",
        "\n",
        "### [PySpark](https://spark.apache.org/docs/latest/api/python/index.html)\n",
        "\n",
        "PySpark Ã© uma interface para Apache Spark em Python. Ele nÃ£o apenas permite que vocÃª escreva aplicativos Spark usando APIs Python, mas tambÃ©m fornece o *shell* PySpark para analisar interativamente seus dados em um ambiente distribuÃ­do. O PySpark oferece suporte Ã  maioria dos recursos do Spark, como Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) e Spark Core.\n",
        "\n",
        "<center><img src=\"https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/img-001.png\"/></center>\n",
        "\n",
        "#### Spark SQL e DataFrame\n",
        "\n",
        "Spark SQL Ã© um mÃ³dulo Spark para processamento de dados estruturados. Ele fornece uma abstraÃ§Ã£o de programaÃ§Ã£o chamada DataFrame e tambÃ©m pode atuar como mecanismo de consulta SQL distribuÃ­do.\n",
        "\n",
        "#### Spark Streaming\n",
        "\n",
        "Executando em cima do Spark, o recurso de *streaming* no Apache Spark possibilita o uso de poderosas aplicaÃ§Ãµes interativas e analÃ­ticas em *streaming* e dados histÃ³ricos, enquanto herda a facilidade de uso do Spark e as caracterÃ­sticas de tolerÃ¢ncia a falhas.\n",
        "\n",
        "#### Spark MLlib\n",
        "\n",
        "ConstruÃ­do sobre o Spark, MLlib Ã© uma biblioteca de aprendizado de mÃ¡quina escalonÃ¡vel que fornece um conjunto uniforme de APIs de alto nÃ­vel que ajudam os usuÃ¡rios a criar e ajustar *pipelines* de aprendizado de mÃ¡quina prÃ¡ticos.\n",
        "\n",
        "#### Spark Core\n",
        "\n",
        "Spark Core Ã© o mecanismo de execuÃ§Ã£o geral subjacente para a plataforma Spark sobre o qual todas as outras funcionalidades sÃ£o construÃ­das. Ele fornece um RDD (*Resilient Distributed Dataset*) e recursos de computaÃ§Ã£o na memÃ³ria.\n",
        "\n",
        "## Utilizando o Spark no Windows\n",
        "\n",
        "[fonte](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
        "\n",
        "#### Passo 1 - Instalando o Java\n",
        "\n",
        "O PySpark requer a instalaÃ§Ã£o do Java na versÃ£o 7 ou superior. Obtenha a versÃ£o mais recente clicando [aqui](https://www.java.com/pt-BR/download/). Para verificar a versÃ£o que estÃ¡ instalada em sua mÃ¡quina execute a seguinte linha de cÃ³digo no seu *prompt*:\n",
        "\n",
        "```\n",
        "java -version\n",
        "```\n",
        "\n",
        "#### Passo 2 - Instalando o Python\n",
        "\n",
        "O Python deve ser instalado em sua versÃ£o 2.6 ou superior. Para obter a versÃ£o mais recente clique [aqui](https://www.python.org/downloads/windows/). Para verificar a versÃ£o do Python que estÃ¡ instalada em sua mÃ¡quina digite o seguinte comando em seu *prompt*:\n",
        "\n",
        "```\n",
        "python --version\n",
        "```\n",
        "\n",
        "#### Passo 3 - Instalando o Apache Spark\n",
        "\n",
        "Selecione a versÃ£o mais estÃ¡vel clicando [aqui](http://spark.apache.org/downloads.html). Na criaÃ§Ã£o deste projeto utilizamos a versÃ£o do Spark **3.1.2** e como tipo de pacote selecionamos **Pre-built for Apache Hadoop 2.7**.\n",
        "\n",
        "Para instalar o Apache Spark nÃ£o Ã© necessÃ¡rio executar um instalador, basta descomprimir os arquivos em uma pasta de sua escolha.\n",
        "\n",
        "<font color=red>Obs.: certifique-se de que o caminho onde os arquivos do Spark foram armazenados nÃ£o contenham espaÃ§os (ex.: **\"C:\\spark\\spark-3.1.2-bin-hadoop2.7\"**).</font>\n",
        "\n",
        "Para testar o funcionamento do Spark execute os comandos abaixo em seu *prompt* de comando. Esses comandos assumem que vocÃª extraiu os arquivos do Spark na pasta **\"C:\\spark\\\"**.\n",
        "\n",
        "```\n",
        "cd C:\\spark\\spark-3.1.2-bin-hadoop2.7\n",
        "```\n",
        "\n",
        "```\n",
        "bin\\pyspark\n",
        "```\n",
        "\n",
        "O comando acima inicia o *shell* do PySpark que permite trabalhar interativamente com o Spark.\n",
        "\n",
        "Para sair basta digitar `exit()` e logo depois presionar *Enter*. Para voltar ao *prompt* pressione *Enter* novamente.\n",
        "\n",
        "#### Passo 4 - Instalando o findspark\n",
        "\n",
        "```\n",
        "pip install findspark\n",
        "```\n",
        "\n",
        "#### Passo 5 - Instalando o winutils\n",
        "\n",
        "Os arquivos do Spark nÃ£o incluem o utilitÃ¡rio **winutils.exe** que Ã© utilizado pelo Spark no Windows. Se nÃ£o informar onde o Spark deve procurar este utilitÃ¡rio, veremos alguns erros no console e tambÃ©m nÃ£o conseguiremos executar *scripts* Python utilizando o utilitÃ¡rio `spark-submit`.\n",
        "\n",
        "FaÃ§a o [download](https://github.com/steveloughran/winutils) para a versÃ£o do Hadoop para a qual sua instalaÃ§Ã£o do Spark foi construÃ­da. Em nosso exemplo foi utilizada a [versÃ£o 2.7](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin). FaÃ§a o *download* apenas do arquivo **winutils.exe**.\n",
        "\n",
        "Crie a pasta **\"hadoop\\bin\"** dentro da pasta que contÃ©m os arquivos do Spark (em nosso exemplo **\"C:\\spark\\spark-3.1.2-bin-hadoop2.7\"**) e copie o arquivo **winutils.exe** para dentro desta pasta.\n",
        "\n",
        "Crie duas variÃ¡veis de ambiente no seu Windows. A primeira chamada **SPARK_HOME** que aponta para a pasta onde os arquivos Spark foram armazenados (em nosso exemplo **\"C:\\spark\\spark-3.1.2-bin-hadoop2.7\"**). A segunda chamada **HADOOP_HOME** que aponta para **%SPARK_HOME%\\hadoop** (assim podemos modificar **SPARK_HOME** sem precisar alterar **HADOOP_HOME**).\n",
        "\n",
        "## Utilizando o Spark no Google Colab\n",
        "\n",
        "Para facilitar o desenvolvimento de nosso projeto neste curso vamos utilizar o Google Colab como ferramenta e para configurar o PySpark basta executar os comandos abaixo na prÃ³pria cÃ©lula do seu *notebook*.\n",
        "\n",
        "# instalar as dependÃªncias\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Carregamento de Dados\n",
        "---\n",
        "\n",
        "## [SparkSession](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html)\n",
        "\n",
        "O ponto de entrada para programar o Spark com a API Dataset e DataFrame.\n",
        "\n",
        "Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrÃ£o de construtor:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"Iniciando com Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n",
        "\n",
        "## Acessando o [Spark UI](https://spark.apache.org/docs/latest/web-ui.html) (Google Colab)\n",
        "\n",
        "\n",
        "\n",
        "[Site ngrok](https://ngrok.com)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## DataFrames com Spark\n",
        "\n",
        "\n",
        "### Interfaces Spark\n",
        "\n",
        "Existem trÃªs interfaces principais do Apache Spark que vocÃª deve conhecer: Resilient Distributed Dataset, DataFrame e Dataset.\n",
        "\n",
        "- **Resilient Distributed Dataset**: A primeira abstraÃ§Ã£o do Apache Spark foi o Resilient Distributed Dataset (RDD). Ã uma interface para uma sequÃªncia de objetos de dados que consiste em um ou mais tipos localizados em uma coleÃ§Ã£o de mÃ¡quinas (um cluster). Os RDDs podem ser criados de vÃ¡rias maneiras e sÃ£o a API de ânÃ­vel mais baixoâ disponÃ­vel. Embora esta seja a estrutura de dados original do Apache Spark, vocÃª deve se concentrar na API DataFrame, que Ã© um superconjunto da funcionalidade RDD. A API RDD estÃ¡ disponÃ­vel nas linguagens Java, Python e Scala.\n",
        "\n",
        "- **DataFrame**: Trata-se de um conceito similar ao DataFrame que vocÃª pode estar familiarizado como o pacote pandas do Python e a linguagem R . A API DataFrame estÃ¡ disponÃ­vel nas linguagens Java, Python, R e Scala.\n",
        "\n",
        "- **Dataset**: uma combinaÃ§Ã£o de DataFrame e RDD. Ele fornece a interface digitada que estÃ¡ disponÃ­vel em RDDs enquanto fornece a conveniÃªncia do DataFrame. A API Dataset estÃ¡ disponÃ­vel nas linguagens Java e Scala.\n",
        "\n",
        "Em muitos cenÃ¡rios, especialmente com as otimizaÃ§Ãµes de desempenho incorporadas em DataFrames e Datasets, nÃ£o serÃ¡ necessÃ¡rio trabalhar com RDDs. Mas Ã© importante entender a abstraÃ§Ã£o RDD porque:\n",
        "\n",
        "- O RDD Ã© a infraestrutura subjacente que permite que o Spark seja executado com tanta rapidez e forneÃ§a a linhagem de dados.\n",
        "\n",
        "- Se vocÃª estiver mergulhando em componentes mais avanÃ§ados do Spark, pode ser necessÃ¡rio usar RDDs.\n",
        "\n",
        "- As visualizaÃ§Ãµes na Spark UI fazem referÃªncia a RDDs.\n",
        "\n",
        "data = [('Zeca','35'), ('Eva', '29')]\n",
        "colNames = ['Nome', 'Idade']\n",
        "\n",
        "%%html\n",
        "<a href=\"your_url_here\">Showing Text</a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Projeto\n",
        "\n",
        "Nosso projeto consiste em ler, manipular, tratar e salvar um conjunto de dados volumosos utilizando como ferramenta o Spark.\n",
        "\n",
        "## Carregamento de dados\n",
        "\n",
        "### Dados PÃºblicos CNPJ\n",
        "#### Receita Federal\n",
        "\n",
        "> [Empresas](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)\n",
        ">\n",
        "> [Estabelecimentos](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)\n",
        ">\n",
        "> [SÃ³cios](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)\n",
        "\n",
        "[Fonte original dos dados](https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/dados-publicos-cnpj)\n",
        "\n",
        "---\n",
        "[property SparkSession.read](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.read.html)\n",
        "\n",
        "[DataFrameReader.csv(*args)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html)\n",
        "\n",
        "\n",
        "### Montando nosso drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/driv..e')\n",
        "\n",
        "### Carregando os dados das empresas\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## FaÃ§a como eu fiz: Estabelecimentos e SÃ³cios\n",
        "\n",
        "### Carregando os dados dos estabelecimentos\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Carregando os dados dos sÃ³cios\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Manipulando os Dados\n",
        "---\n",
        "\n",
        "## OperaÃ§Ãµes bÃ¡sicas\n",
        "\n",
        "\n",
        "\n",
        "### Renomeando as colunas do DataFrame\n",
        "\n",
        "empresasColNames = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']\n",
        "\n",
        "\n",
        "\n",
        "estabsColNames = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']\n",
        "\n",
        "\n",
        "\n",
        "sociosColNames = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']\n",
        "\n",
        "\n",
        "\n",
        "## Analisando os dados\n",
        "\n",
        "[Data Types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Modificando os tipos de dados\n",
        "\n",
        "[Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions)\n",
        "\n",
        "[withColumn](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)\n",
        "\n",
        "### Convertendo String â Double\n",
        "\n",
        "#### `StringType â DoubleType`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Convertendo String â Date\n",
        "\n",
        "#### `StringType â DateType`\n",
        "\n",
        "[Datetime Patterns](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n",
        "\n",
        "df = spark.createDataFrame([(20200924,), (20201022,), (20210215,)], ['data'])\n",
        "df.toPandas()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SeleÃ§Ãµes e consultas\n",
        "---\n",
        "\n",
        "## Selecionando informaÃ§Ãµes\n",
        "\n",
        "[DataFrame.select(*cols)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## FaÃ§a como eu fiz\n",
        "\n",
        "\n",
        "\n",
        "## Identificando valores nulos\n",
        "\n",
        "df = spark.createDataFrame([(1,), (2,), (3,), (None,)], ['data'])\n",
        "df.toPandas()\n",
        "\n",
        "df.show()\n",
        "\n",
        "5d df = spark.createDataFrame([(1.,), (2.,), (3.,), (float('nan'),)], ['data'])\n",
        "df.toPandas()\n",
        "\n",
        "df.show()\n",
        "\n",
        "df = spark.createDataFrame([('1',), ('2',), ('3',), (None,)], ['data'])\n",
        "df.toPandas()\n",
        "\n",
        "df.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Ordenando os dados\n",
        "\n",
        "[DataFrame.orderBy(*cols, **kwargs)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Filtrando os dados\n",
        "\n",
        "[DataFrame.where(condition)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.where.html) ou [DataFrame.filter(condition)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html)\n",
        "\n",
        "\n",
        "\n",
        "#  Fabio\n",
        "\n",
        "# %%html\n",
        "<a href=\"your_url_here\">Showing Text</a>\n",
        "\n",
        "\n",
        "# O comando LIKE\n",
        "\n",
        "[Column.like(other)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.like.html)\n",
        "\n",
        "df = spark.createDataFrame([('RESTAURANTE DO RUI',), ('Juca restaurantes ltda',), ('Joca Restaurante',)], ['data'])\n",
        "df.toPandas()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# AgregaÃ§Ãµes e JunÃ§Ãµes\n",
        "---\n",
        "\n",
        "[DataFrame.groupBy(*cols)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)\n",
        "\n",
        "[DataFrame.agg(*exprs)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.agg.html)\n",
        "\n",
        "[DataFrame.summary(*statistics)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.summary.html)\n",
        "\n",
        "> FunÃ§Ãµes:\n",
        "[approx_count_distinct](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.approx_count_distinct.html) |\n",
        "[avg](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.avg.html) |\n",
        "[collect_list](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.collect_list.html) |\n",
        "[collect_set](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.collect_set.html) |\n",
        "[countDistinct](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.countDistinct.html) |\n",
        "[count](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.count.html) |\n",
        "[grouping](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.grouping.html) |\n",
        "[first](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.first.html) |\n",
        "[last](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.last.html) |\n",
        "[kurtosis](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.kurtosis.html) |\n",
        "[max](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.max.html) |\n",
        "[min](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.min.html) |\n",
        "[mean](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.mean.html) |\n",
        "[skewness](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.skewness.html) |\n",
        "[stddev ou stddev_samp](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.stddev.html) |\n",
        "[stddev_pop](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.stddev_pop.html) |\n",
        "[sum](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.sum.html) |\n",
        "[sumDistinct](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.sumDistinct.html) |\n",
        "[variance ou var_samp](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.variance.html) |\n",
        "[var_pop](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.var_pop.html)\n",
        "\n",
        "## Sumarizando os dados\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Juntando DataFrames - Joins\n",
        "\n",
        "[DataFrame.join(*args)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## SparkSQL\n",
        "\n",
        "[SparkSession.sql(sqlQuery)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.sql.html)\n",
        "\n",
        "Para saber mais sobre performance: [Artigo - Spark RDDs vs DataFrames vs SparkSQL](https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Formas de Armazenamento\n",
        "---\n",
        "\n",
        "## Arquivos CSV\n",
        "\n",
        "[property DataFrame.write](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.write.html)\n",
        "\n",
        "[DataFrameWriter.csv(*args)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## FaÃ§a como eu fiz\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Arquivos PARQUET\n",
        "\n",
        "[Apache Parquet](https://parquet.apache.org/)\n",
        "\n",
        "[DataFrameWriter.parquet(*args)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.parquet.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Particionamento dos dados\n",
        "\n",
        "[DataFrameWriter.partitionBy(*cols)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "919d4e30",
      "metadata": {
        "id": "919d4e30"
      },
      "outputs": [],
      "source": [
        "socios\\\n",
        "    .select(f.year('data_de_entrada_sociedade').alias('ano_de_entrada'))\\\n",
        "    .where('ano_de_entrada >= 200')\\\n",
        "    .groupBy('ano_de_entrada')\\\n",
        "    .count()\\\n",
        "    .orderBy('ano_de_entrada', ascending=True)\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edfe9ee0",
      "metadata": {
        "id": "edfe9ee0"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .select('cnpj_basico', 'porte_da_empresa', 'capital_social_da_empresa')\\\n",
        "    .groupBy('porte_da_empresa')\\\n",
        "    .agg(\n",
        "        f.avg('capital_social_da_empresa').alias('capital_social_medio'),\n",
        "    )\\\n",
        "    .orderBy('porte_da_empresa', ascending=True)\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48daf20c",
      "metadata": {
        "id": "48daf20c"
      },
      "outputs": [],
      "source": [
        "empresas\\\n",
        "    .select('capital_social_da_empresa')\\\n",
        "    .summary()\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adc13111",
      "metadata": {
        "id": "adc13111"
      },
      "source": [
        "## juntar dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47846a8b",
      "metadata": {
        "id": "47846a8b"
      },
      "outputs": [],
      "source": [
        "produtos = spark.createDataFrame(\n",
        "    [\n",
        "        ('1', 'Bebidas', 'Ãgua mineral'),\n",
        "        ('2', 'Limpeza', 'SabÃ£o em PÃ³'),\n",
        "        ('3', 'Frios', 'Queijo'),\n",
        "        ('4', 'Bebidas', 'Refrigerante'),\n",
        "        ('5', 'Pet', 'RaÃ§Ã£o para cÃ£es'),\n",
        "    ],['id', 'cat', 'prod']\n",
        ")\n",
        "\n",
        "impostos = spark.createDataFrame(\n",
        "    [\n",
        "        ('Bevidas', '0.15'),\n",
        "        ('Limpeza', '0.05'),\n",
        "        ('Frios', '0.065'),\n",
        "        ('Carnes', '0.08'),\n",
        "    ],['cat', 'tax']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "281c9451",
      "metadata": {
        "id": "281c9451"
      },
      "outputs": [],
      "source": [
        "produtos.join(impostos, 'cat', how='inner')\\\n",
        "    .sort('id')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863d24da",
      "metadata": {
        "id": "863d24da"
      },
      "outputs": [],
      "source": [
        "produtos.join(impostos, 'cat', how='left')\\\n",
        "    .sort('id')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcee85cb",
      "metadata": {
        "id": "dcee85cb"
      },
      "outputs": [],
      "source": [
        "produtos.join(impostos, 'cat', how='outer')\\\n",
        "    .sort('id')\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3f19e3",
      "metadata": {
        "id": "2d3f19e3"
      },
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a344b3b",
      "metadata": {
        "id": "2a344b3b"
      },
      "outputs": [],
      "source": [
        "empresas_join = estabelecimentos.join(empresas, \"cnpj_basico\", how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5797c190",
      "metadata": {
        "id": "5797c190"
      },
      "outputs": [],
      "source": [
        "empresas.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce89d9bc",
      "metadata": {
        "id": "ce89d9bc"
      },
      "outputs": [],
      "source": [
        "freq = empresas_join\\\n",
        "    .select(\n",
        "        'cnpj_basico',\n",
        "        f.year('data_de_inicio_atividade').alias('data_de_inicio')\n",
        "    )\\\n",
        "    .where('data_de_inicio >=2010')\\\n",
        "    .groupBy('data_de_inicio')\\\n",
        "    .agg(f.count('cnpj_basico').alias('frequencia'))\\\n",
        "    .orderBy('data_de_inicio', ascending = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd445fc8",
      "metadata": {
        "id": "cd445fc8"
      },
      "outputs": [],
      "source": [
        "freq.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f927e8",
      "metadata": {
        "id": "72f927e8"
      },
      "outputs": [],
      "source": [
        "freq.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17dcb03",
      "metadata": {
        "id": "b17dcb03"
      },
      "outputs": [],
      "source": [
        "freq.union(\n",
        "    freq.select(\n",
        "        f.lit('Total').alias(\"data_de_inicio\"),\n",
        "        f.sum(freq.frequencia).alias('frequencia')\n",
        "    )\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5749b912",
      "metadata": {
        "id": "5749b912"
      },
      "outputs": [],
      "source": [
        "empresas.createOrReplaceTempView('empresasview')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23000a23",
      "metadata": {
        "id": "23000a23"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"select * from empresasview\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c542fd81",
      "metadata": {
        "id": "c542fd81"
      },
      "outputs": [],
      "source": [
        "spark\\\n",
        "    .sql(\"\"\"\n",
        "        select *\n",
        "        from empresasview where capital_social_da_empresa >=50\n",
        "        \"\"\")\\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a178aa6",
      "metadata": {
        "id": "3a178aa6"
      },
      "outputs": [],
      "source": [
        "spark\\\n",
        "    .sql(\"\"\"\n",
        "        select porte_da_empresa, MEAN(capital_social_da_empresa) AS media\n",
        "        from empresasview\n",
        "        Group by porte_da_empresa\n",
        "        \"\"\")\\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd55eefa",
      "metadata": {
        "id": "cd55eefa"
      },
      "outputs": [],
      "source": [
        "empresas.write.csv(\n",
        "    path=empresa_path+'/pastacsv',\n",
        "    mode='overwrite',\n",
        "    sep=';',\n",
        "    header=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28982ba4",
      "metadata": {
        "id": "28982ba4"
      },
      "outputs": [],
      "source": [
        "empresas2 = spark.read.csv(empresa_path+'/pastacsv', sep=';', inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "431c2487",
      "metadata": {
        "id": "431c2487"
      },
      "outputs": [],
      "source": [
        "empresas2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163976a4",
      "metadata": {
        "id": "163976a4"
      },
      "outputs": [],
      "source": [
        "empresas.write.parquet(\n",
        "    path=empresa_path+'/parquet',\n",
        "    mode='overwrite'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0c9ec9",
      "metadata": {
        "id": "1a0c9ec9"
      },
      "outputs": [],
      "source": [
        "empresas3= spark.read.parquet(empresa_path+'/parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f02d35c",
      "metadata": {
        "id": "5f02d35c"
      },
      "outputs": [],
      "source": [
        "empresas3.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0b5864",
      "metadata": {
        "id": "3c0b5864"
      },
      "outputs": [],
      "source": [
        "empresas.coalesce(1).write.csv(\n",
        "    path=empresa_path+'/csvunico',\n",
        "    mode='overwrite',\n",
        "    sep=';',\n",
        "    header=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464e4385",
      "metadata": {
        "id": "464e4385"
      },
      "outputs": [],
      "source": [
        "empresas.write.parquet(\n",
        "    path=empresa_path+'/pastaunica',\n",
        "    mode='overwrite',\n",
        "    partitionBy='porte_da_empresa'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d241f84",
      "metadata": {
        "id": "7d241f84"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}